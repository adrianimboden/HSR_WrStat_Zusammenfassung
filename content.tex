%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kombinatorik (Zählen)}
\subsection{Produktregel: Die Für–jedes–gibt–es–Regel (FJGE)}
Fur jede der $n_1$ Möglichkeiten gibt es eine von der
ersten Position unabhängige Anzahl $n_2$ Möglichkeiten für den Rest.
\[ = n_1 \cdot n_2 \text{ Möglichkeiten } \]

\subsection{Permutationen: Reihenfolge}
Auf wieviele Arten kann man $n$ Objekte anordnen? (Anzahl Anordnungen)
\[ = n! \text{ Arten} \]

\subsection{Kombinationen: Auswahl}
Auf wieviele Arten kann man $k$ Objekte aus $n$ auswählen?
\[ = C^n_k=\binom{n}{k} = \frac{n!}{k!(n-k)!} \text{ Arten} \]
Dieser "`Binomialkoeffizient"' lässt sich auf dem Taschenrechner
TI-36XII mit \texttt{n nCr k} (unter \texttt{PRB}), mit dem Voyage 200
mit \texttt{nCr(n,k)}, in Sage mit \texttt{binomial(n,k)} und in 
Octave/Matlab mit \texttt{nchoosek(n,k)} berechnen.

Auf wieviele Arten kann man $k$ Mal eine Auswahl aus $n$ Objekten
treffen?
\[ = n^k \text{ Arten} \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ereignisse und Wahrscheinlichkeit}
\subsection{Wahrscheinlichkeitsexperimente}
\begin{itemize}
  \item Alle möglichen Versuchsausgänge $\Omega$
  \item Elementarereignis $\omega \in \Omega$
  \item Ereignis $A \subset \Omega$
  \item Verschiedene Elementarereignisse $\omega \in A$
  \item Ereignis $A$ tritt ein $\Leftrightarrow \omega \in A$
  \item Wahrscheinlichkeit dass Ereignis $A$ eintritt ist $P(A)$
\end{itemize}

\subsection{Eigenschaften von Wahrscheinlichkeiten}
\begin{itemize}
  \item Laplace-Ereignisse: Alle Elementarereignisse haben die gleiche
    Wahrscheinlichkeit
  \item $P(A) = \frac{|A|}{|\Omega|}$: Wahrscheinlichkeit von
    Elementarereignis $A$
  \item $0 \le P(A) \le 1$: Wahrscheinlichkeit ist immer zwischen $0$
    und $1$
  \item $P(A) < P(B)$: Die Wahrscheinlichkeit für das Ereignis $A$ ist
    kleiner als für $B$
  \item $P(\Omega) = 1$: Das sichere Ereignis tritt immer ein
  \item $P(\emptyset) = 0$: Das unmöglich Ereignis tritt nie ein
\end{itemize}

\subsection{Rechnen mit Wahrscheinlichkeiten}
\begin{itemize}
  \item $P(A \cap B)$: Ereignis $A$ und Ereignis $B$ tritt ein
    \begin{itemize}
      \item Falls unabhängig: $= P(A) \cdot P(B)$
      \item Falls abhängig: Nicht alleine aus $P(A)$ und $P(B)$
        berechenbar!
    \end{itemize}
  \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$\footnote{Ein-
    Ausschaltformel}: Ereignis $A$ oder Ereignis $B$ tritt ein
    \begin{itemize}
      \item Falls sich $A$ und $B$ nicht überschneiden\footnote{
        Paarweise disjunkt: $A$ und $B$ treffen nicht gleichzeitig ein}:
        $= P(A) + P(B)$
      \item Drei Ereignisse: $P(A \cup B \cup C) \\ = P(A) + P(B) + P(C)
        - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$
    \end{itemize}
  \item $P(A \setminus B) = 1 - P(A)$: Ereignis $A$ tritt ein, aber ohne
    Ereignis $B$
  \item $P(\bar{A}) = P(\Omega \setminus A) = 1 - P(A)$: Ereignis $A$
    tritt nicht ein
  \begin{itemize}
    \item Bedingte Wahrscheinlichkeit: $P(\bar{A}|B) = 1 - P(A|B)$
  \end{itemize}
\end{itemize}

\subsection{Bedingte Wahrscheinlichkeit}
Wahrscheinlichkeit, dass $A$ eintritt, wenn wir schon wissen, dass $B$
eingetreten ist.\footnote{Somit gilt auch: $P(A) = P(A|\Omega)$}
\[ P(A|B) = \frac{P(A \cap B)}{P(B)} \]
Falls $A$ und $B$ unabhängig sind:
\[ P(A|B) = \frac{P(A) P(B)}{P(B)} = P(A) \]

\subsection{Satz der totalen Wahrscheinlichkeit}
Aus Einzelfällen kann man die Gesamtsituation zusammenstellen:
\[ P(A) = \sum_{i=1}^{n}P(A|B_i) \cdot P(B_i) \]

\subsection{Satz von Bayes}
Mit dem Satz von Bayes kann man die Schlussrichtung umkehren
\footnote{Weil $P(A|B) \cdot P(B) = P(A \cap B) = P(B|A) \cdot P(A)$}:
\[ P(A|B) = P(B|A) \cdot \frac{P(A)}{P(B)} \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Erwartungswert und Varianz}
\subsection{Begriffe}
\begin{itemize}
  \item Zufallsvariable $X$ ordnet Elementarereignissen $\omega$ Werte
    zu: $X: \Omega \rightarrow \mathbb{R}$
  \item Erwartungswert $E(X) = \sum_{Werte} \text{Wert } \cdot
    \text{Wahrscheinlichkeit } = \sum_i w_i \cdot P(w_i)$
  \item Empirischer Erwartungswert = arithmetisches Mittel:
    $E(X) = \frac{1}{n} \sum_{i=1}^n x_i$
\end{itemize}
\subsubsection{Spezielle Zufallsvariable}
Charakteristische Funktion von $A$:
\[ \chi_A = \begin{cases}1 & w \in A \\ 0 & sonst \end{cases} \]
\[ E(X) = 0 \cdot P(\bar{A}) + 1 \cdot P(A) = P(A) \]

\subsection{Rechnen mit Erwartungswerten}
\begin{itemize}
  \item Multiplikation mit einem Faktor $E(\lambda X) = \lambda E(X)$
  \item Addition zweier Zufallswerte: $E(X + Y) = E(X) + E(Y)$
  \item Produkt zweier Zufallswerte: $E(X \cdot Y) \ne E(X) \cdot E(Y)$
  \begin{itemize}
    \item Potenzieren (immer abhängig): $E(X^2) \ne E(X)^2$
    \item Wenn die zwei Werte sich nicht beeinflussen
      (unabhängig): $E(X \cdot Y) = E(X) \cdot E(Y)$
  \end{itemize}
\end{itemize}

\subsection{Streubreite (Varianz)}
\begin{itemize}
  \item Varianz $\text{var}(x) = E((X - E(X))^2) = E(X^2) - E(X)^2$
  \item Abstand ($\pm$) zum Erwartungswert: $\sqrt{\text{var}(x)}$
\end{itemize}
