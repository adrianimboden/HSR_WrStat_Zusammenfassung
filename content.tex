%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kombinatorik (Zählen)}
\subsection{Produktregel: Die Für–jedes–gibt–es–Regel (FJGE)}
Fur jede der $n_1$ Möglichkeiten gibt es eine von der
ersten Position unabhängige Anzahl $n_2$ Möglichkeiten für den Rest.
\[ = n_1 \cdot n_2 \text{ Möglichkeiten } \]

\subsection{Permutationen: Reihenfolge}
Auf wieviele Arten kann man $n$ Objekte anordnen? (Anzahl Anordnungen)
\[ = n! \text{ Arten} \]

\subsection{Kombinationen: Auswahl}
Auf wieviele Arten kann man $k$ Objekte aus $n$ auswählen?
\[ = C^n_k=\binom{n}{k} = \frac{n!}{k!(n-k)!} \text{ Arten} \]
Dieser "`Binomialkoeffizient"' lässt sich auf dem Taschenrechner
TI-36XII mit \texttt{n nCr k} (unter \texttt{PRB}), mit dem Voyage 200
mit \texttt{nCr(n,k)}, in Sage mit \texttt{binomial(n,k)} und in 
Octave/Matlab mit \texttt{nchoosek(n,k)} berechnen.

Auf wieviele Arten kann man $k$ Mal eine Auswahl aus $n$ Objekten
treffen?
\[ = n^k \text{ Arten} \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ereignisse und Wahrscheinlichkeit}
\subsection{Wahrscheinlichkeitsexperimente}
\begin{itemize}
  \item Alle möglichen Versuchsausgänge $\Omega$
  \item Elementarereignis $\omega \in \Omega$
  \item Ereignis $A \subset \Omega$
  \item Verschiedene Elementarereignisse $\omega \in A$
  \item Ereignis $A$ tritt ein $\Leftrightarrow \omega \in A$
  \item Wahrscheinlichkeit dass Ereignis $A$ eintritt ist $P(A)$
\end{itemize}

\subsection{Eigenschaften von Wahrscheinlichkeiten}
\begin{itemize}
  \item Laplace-Ereignisse: Alle Elementarereignisse haben die gleiche
    Wahrscheinlichkeit
  \item $P(A) = \frac{|A|}{|\Omega|}$: Wahrscheinlichkeit von
    Elementarereignis $A$
  \item $0 \le P(A) \le 1$: Wahrscheinlichkeit ist immer zwischen $0$
    und $1$
  \item $P(A) < P(B)$: Die Wahrscheinlichkeit für das Ereignis $A$ ist
    kleiner als für $B$
  \item $P(\Omega) = 1$: Das sichere Ereignis tritt immer ein
  \item $P(\emptyset) = 0$: Das unmöglich Ereignis tritt nie ein
\end{itemize}

\subsection{Rechnen mit Wahrscheinlichkeiten}
\begin{itemize}
  \item $P(A \cap B)$: Ereignis $A$ und Ereignis $B$ tritt ein
    \begin{itemize}
      \item Falls unabhängig: $= P(A) \cdot P(B)$
      \item Falls abhängig: Nicht alleine aus $P(A)$ und $P(B)$
        berechenbar!
    \end{itemize}
  \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$\footnote{Ein-
    Ausschaltformel}: Ereignis $A$ oder Ereignis $B$ tritt ein
    \begin{itemize}
      \item Falls sich $A$ und $B$ nicht überschneiden\footnote{
        Paarweise disjunkt: $A$ und $B$ treffen nicht gleichzeitig ein}:
        $= P(A) + P(B)$
      \item Drei Ereignisse: $P(A \cup B \cup C) \\ = P(A) + P(B) + P(C)
        - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$
    \end{itemize}
  \item $P(A \setminus B) = 1 - P(A)$: Ereignis $A$ tritt ein, aber ohne
    Ereignis $B$
  \item $P(\bar{A}) = P(\Omega \setminus A) = 1 - P(A)$: Ereignis $A$
    tritt nicht ein
  \begin{itemize}
    \item Bedingte Wahrscheinlichkeit: $P(\bar{A}|B) = 1 - P(A|B)$
  \end{itemize}
\end{itemize}

\subsection{Bedingte Wahrscheinlichkeit}
Wahrscheinlichkeit, dass $A$ eintritt, wenn wir schon wissen, dass $B$
eingetreten ist.\footnote{Somit gilt auch: $P(A) = P(A|\Omega)$}
\[ P(A|B) = \frac{P(A \cap B)}{P(B)} \]
Falls $A$ und $B$ unabhängig sind:
\[ P(A|B) = \frac{P(A) P(B)}{P(B)} = P(A) \]

\subsection{Satz der totalen Wahrscheinlichkeit}
Aus Einzelfällen kann man die Gesamtsituation zusammenstellen:
\[ P(A) = \sum_{i=1}^{n}P(A|B_i) \cdot P(B_i) \]

\subsection{Satz von Bayes}
Mit dem Satz von Bayes kann man die Schlussrichtung umkehren
\footnote{Weil $P(A|B) \cdot P(B) = P(A \cap B) = P(B|A) \cdot P(A)$}:
\[ P(A|B) = P(B|A) \cdot \frac{P(A)}{P(B)} \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Erwartungswert und Varianz}
\subsection{Erwartungswert}
\subsubsection{Begriffe}
\begin{itemize}
  \item Zufallsvariable $X$ ordnet Elementarereignissen $\omega$ Werte
    zu: $X: \Omega \rightarrow \mathbb{R}$
  \item Erwartungswert (Zufallsvariable im Mittel): \\
    $E(X) = \sum_{Werte} \text{Wert } \cdot
    \text{Wahrscheinlichkeit } = \sum_i w_i \cdot P(w_i)$
  \item Empirischer Erwartungswert = arithmetisches Mittel:
    $E(X) = \frac{1}{n} \sum_{i=1}^n x_i$
\end{itemize}
\subsubsection{Spezielle Zufallsvariable}
Charakteristische Funktion von $A$:
\[ \chi_A = \begin{cases}1 & w \in A \\ 0 & sonst \end{cases} \]
\[ E(X) = 0 \cdot P(\bar{A}) + 1 \cdot P(A) = P(A) \]

\subsubsection{Rechnen mit Erwartungswerten}
\begin{itemize}
  \item Multiplikation mit einem Faktor: $E(\lambda X) = \lambda E(X)$
  \item Addition zweier Zufallswerte: $E(X + Y) = E(X) + E(Y)$
  \item Produkt zweier Zufallswerte: $E(X \cdot Y) \ne E(X) \cdot E(Y)$
  \begin{itemize}
    \item Potenzieren (immer abhängig): $E(X^2) \ne E(X)^2$
    \item Wenn die zwei Werte sich nicht beeinflussen
      (unabhängig): $E(X \cdot Y) = E(X) \cdot E(Y)$
  \end{itemize}
  \item Erwartungswert einer Konstante $c$: $E(c) = c$
\end{itemize}

\subsection{Varianz (Streubreite)}
\subsubsection{Begriffe}
\begin{itemize}
  \item Varianz (Mass für die mittlere Abweichung):
    $\text{var}(x) = E((X - E(X))^2) = E(X^2) - E(X)^2$
  \item Kovarianz (Verschiebungssatz):
    $\text{cov}(X,Y) = E(XY) - E(X)E(Y)$
  \item Abstand ($\pm$) zum Erwartungswert: $\sqrt{\text{var}(x)}$
\end{itemize}
\subsubsection{Rechenregeln für Varianz}
\begin{itemize}
  \item Multiplikation mit einem Faktor:
    $\text{var}(\lambda X) = \lambda^2 \text{var}(X)$
  \item Addition (nur wenn $X$ und $Y$ unabhängig):
    $\text{var}(X+Y) = \text{var}(X) + \text{var}(Y)$
    \begin{itemize}
      \item Achtung ($(-x)^2 = +x$): $\text{var}(X-Y) = \text{var}(X)
        + \text{var}(-Y) = \text{var}(X) + \text{var}(Y)$
    \end{itemize}
  \item Multiplikation: $\text{var}(X \cdot Y) = \text{var}(X)
    \text{var}(Y) + \text{var}(Y)E(X)^2 + \text{var}(X)E(Y)^2$
  \item Varianz einer Konstante $c$: $\text{var}(c) = 0$
  \item Kovarianz als Verallgemeinerung der Varianz:
    $\text{var}(X) = \text{cov}(X,X)$
\end{itemize}

\subsection{Mittelwert}
\subsubsection{Begriffe}
\begin{itemize}
  \item Erwartungswert der Zufallsvariablen $X$: $\mu = E(X)$
  \item Varianz: Abweichung der Zufallsvariabeln von ihrem
    Erwartungswert: $X -\mu$
  \item Wie häufig überschreitet die Abweichung $\varepsilon$?
  \item Wie wahrscheinlich ist es, dass die Abweichung gross ist?
    $P(|X - \mu| > \varepsilon)$
  \item Faustregel: 10 mal mehr Genauigkeit = 100 mal mehr Arbeit.
\end{itemize}
\subsubsection{Ungleichung von Tschebyscheff}
Wahrscheinlichkeit, dass Zufallsvariable $X$ um mehr als $\varepsilon$
vom Erwartungswert abweicht:
\[P(X|-\mu| > \varepsilon) \le \frac{\text{var}(X)}{\varepsilon^2} \]
\subsubsection{Wie gut ist der Mittelwert?}
\begin{itemize}
  \item Mittelwert: $M_n = \frac{X_1 + X_2 + \ldots + X_n}{n}$
  \item Erwartungswert: $E(X_i) = E\left( \frac{X_1 + X_2 + \ldots +
  X_n}{n}\right) =  \frac{E(X_1) + E(X_2) + \ldots + E(X_n)}{n} = \mu$
  \item Varianz: $\text{var}(X_i) = \sigma^2$
  \item Varianz: $\text{var}(M_n) = \frac{1}{n^2} \sum_{i=1}^n
  \text{var}(X_i) = \frac{\sigma^2}{n}$
\end{itemize}
\paragraph{Bernoullis Gesetz der grossen Zahlen}
Die Wahrscheinlichkeit, dass der Mittelwert von $n$ unabhängigen
Zufallsvariabeln mit Mittelwert $\mu$ und Varianz $\sigma^2$ mehr als
$\varepsilon$ von $\mu$ abweicht, ist:
\[P(|X-\mu| > \varepsilon) \le \frac{\text{var}(X)}{\varepsilon^2} \]
Die Abweichung ist unwahrscheinlich, wenn:
\begin{itemize}
  \item Varianz ist klein (genaues Messgerät)
  \item $\varepsilon$ ist gross (man ist toleranter)
  \item $n$ ist gross (viele Messungen)
\end{itemize}

\subsection{Lineare Regression}
Seien $X$ und $Y$ zwei reelle Zufallsvariablen. Die Gerade mit der
Gleichung $y = ax + b$ minimiert die Varianz $\text{var}(aX + b -Y)$
genau dann, wenn
\begin{align*}
  a & = \frac{\text{cov}(X,Y)}{\text{var}(X)} = \frac{E(XY)
        - E(X)E(Y)}{E(X^2) - E(X)^2} \\
  b & = E(Y) - E(X)a
\end{align*}
Die Regression ist umso genauer, je näher der Regressionskoeffizient $r$
bei $\pm 1$ liegt. Zudem hat $r$ immer das gleiche Vorzeichen wie die
Steitung der Regressionsgerade.
\[ r = \frac{\text{cov}(X,Y)}{\sqrt{\text{var}(X)\text{var}(Y)}} \]

Zur Berechnung der Regression eignet sich folgende Tabelle sehr gut,
damit man alle Werte im Überblick hat:

\begin{tabular}{|l|l|l|l|l|l|}
  \hline
  $i$    & $x_i$    & $y_i$    & ${x_i}^2$    & ${y_i}^2$  & $x_iy_i$ \\
  \hline
  \vdots & \vdots   & \vdots   & \vdots       & \vdots       & \vdots \\
  \hline
  $E$ & $E(x_i)$ & $E(y_i)$ & $E({x_i}^2)$ & $E({y_i}^2)$ & $E(x_iy_i)$ \\
  \hline
\end{tabular}
\begin{itemize}
  \item Punkt auf X-Achse: $x_i$
  \item Punkt auf Y-Achse: $y_i$
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Wahrscheinlichkeitsverteilung}
\subsection{Begriffe}
\begin{itemize}
  \item Verteilungsfunktion $F$ von $X$: $F_X(x)$
  \item Ableitung von $F$ (Wahrscheinlichkeitsdichte / Dichtefunktion):
    $F'(x)$
  \item Formen der Zufallsvariabeln
  \begin{itemize}
    \item Diskret: $F$ ist stückweise konstant (Treppenstufen)
    \item Stetig: $F$ ist ist stetig
  \end{itemize}
  \item Eigenschaften der Verteilungsfunktion
  \begin{itemize}
    \item Verteilfunktion zwischen $0$ und $1$: $0 \le F_X(x) \le 1$
    \item Grenzwerte: $\lim_{x \to -\infty} F_X(x) = 0$ 
      und $\lim_{x \to \infty} F_X(x) = 1$
    \item $F$ ist monoton wachsend: $a \le b \Rightarrow F(a) \le F(b)$
  \end{itemize}
\end{itemize}

\subsection{Wahrscheinlichkeit berechnen}
\begin{itemize}
   \item $F_X(x) = P(X \le x)$
\end{itemize}

\subsection{Erwartungswert berechnen}
\begin{itemize}
  \item $F$ ist diskret: % TODO
  \item $F$ ist stetig: $E(X) = \int_{-\infty}^{\infty}x \cdot F'(x)dx$
\end{itemize}

\subsection{Rechenregeln für die Verteilungsfunktion}
\begin{itemize}
  \item Multiplikation mit einem Faktor $\lambda > 0$: $F_{\lambda X}(x)=
    P(\lambda X \le x) = P(X \le \frac{x}{\lambda}) =
    F_X(\frac{x}{\lambda})$
  \item Addition: $F_{X+a}(x) = P(X+a \le x) = P(X \le x-a) = F_X(x-a)$
  \item Quadrieren: $F_{X^2}(x) = P(X^2 \le x) = P(X \le \sqrt{x}) =
    F_X(\sqrt{x})$
  \item Addition zweier diskreten Zufallszahlen:
    $F_{X+Y}(x) = \sum_x P(X=x)P(Y=z-x)$
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Katalog von Wahrscheinlichkeitsverteilungen}
\subsection{Gleichverteilung (stetig)}
\begin{description}
  \item[Anwendung] Zufallszahlen
  \item[Verteilungsfunktion] \[F(x) = \begin{cases} 0 & x < a \\
    \frac{x-a}{b-a} & x \in [a,b] \\ 1 & x > b\end{cases}\]
  \item[Wahrscheinlichkeitsdichte] \[\phi(x) = \begin{cases} 0 & x < a \\
    \frac{1}{a} & x \in [a,b] \\ 0 & x > b\end{cases}\]
  \item[Erwartungswert] Sei $X$ eine in $[a,b]$ gleichverteilte
    Zufallsvariable, dann gilt \[E(X) = \frac{a+b}{2}\]
  \item[Varianz] \[\text{var}(X) = \frac{(a-b)^2}{12}\]
  \item[Wahrscheinlichkeit einer grossen Abweichung] Für $\varepsilon >
  \frac{b-a}{2}$ ist die Wahrscheinlichkeit $P(|X-\mu| > \varepsilon)$
    einer Abweichung vom Erwartungswert $\mu = E(X)$ natürlich 0, aber für
    kleinere $\varepsilon$ ergibt sich
    \[P(|X-\mu| > \varepsilon) = 1 - \frac{2\varepsilon}{b-a}\]
\end{description}

\subsection{Exponentialverteilung (stetig)}
\begin{description}
  \item[Anwendung] Prozess ohne Erinnerungsvermögen, Radioaktivität
  \item[Dichtefunktion] \[\phi(x) = \begin{cases}0 & x < 0 \\ ae^{-ax} &
    x \geq 0\end{cases}\]
  \item[Verteilungsfuntkion] \[F(x) = \begin{cases}0 & x < 0 \\
    1-e^{-ax} & x \geq 0\end{cases}\]
  \item[Erwartungswert] \[E(X) = \frac{1}{a}\]
  \item[Varianz] \[\text{var}(X) = \frac{1}{a^2}\]
  \item[Wahrscheinlichkeit grosser Abweichung] Für eine
    exponentialverteilte Zufallsvariable mit dem Erwartungswert
    $\frac{1}{a}$ ist die Wahrscheinlichkeit einer Abweichung
    $\varepsilon$ vom Erwartungswert \[P(|X-\frac{1}{a}| > \varepsilon) =
    \begin{cases}e^{-a\varepsilon-1} & \varepsilon > \frac{1}{a} \\
    1 - e^{a\varepsilon-1} + e^{-a\varepsilon-1} & \varepsilon \leq
    \frac{1}{a}\end{cases}\]
\end{description}

\subsection{Binomialverteilung (diskret)}
\begin{description}
  \item[Anwendungen] Anzahl Einheiten eines Bernoullie-Ereignisses bei
    $n$ Wiederholungen ($2$ mögliche Versuchsausgänge).
  \item[Wahrscheinlichkeit] Eine Zufallsvariable mit diskreten Werten
    $k \in \{0, \dots, n\}$ heisst binomialverteilt zum Parameter $p$,
    wenn folgendes die Wahrscheinlichkeit des Wertes $k$ ist.
  \[P(X = k) = \binom{n}{k} p^k(1-p)^{n-k}\]
  \item[Erwartungswert] \[E(X) = pn\]
  \item[Varianz] \[\text{var}(X) = np(1-p)\]
    Maximale Varianz wird erreicht bei $p = \frac{1}{2}$
\end{description}

\subsection{Poissonverteilung (diskret)}
\begin{description}
  \item[Anwendung] Anzahl Eintreten in einem Zeitintervall, wenn die
    Zeitabstände exponentiell verteilt sind.
  \item[Verteilungsfunktion] Sind $(X_i)1 \leq i \leq k$
  exponentialverteilte, unabhängige Zufallsvariablen, dann gilt für die
  Summe:
  \[F_{X_i+\dots+x_k}(x) = \begin{cases}1 - e^{-ax} \sum_{i=0}^{k-1}
  \frac{(ax)^i}{i!} & x \geq 0 \\ 0 & x < 0\end{cases}\]
  \item[Wahrscheinlichkeitsdichte]
  \[\phi_{X_i+\dots+x_k}(x) = \begin{cases}a^k \frac{k^{k-1}}{(k-1)!}
  e^{-ax} & x \geq 0 \\ 0 & x < 0\end{cases}\]
  \item[Wahrscheinlichkeit] Beschreibt für $\lambda = ax$ die
    Wahrscheinlichkeit, dass in einem Zeitintervall $[0, x]$ genau $k$
    Ereignisse eintregen, wenn die Zeit zwischen den Ereignissen
    exponentialverteilt ist mit der Dichte $ae^{-ax}$ ($\lambda$ = Rate,
    in der Ereignisse auftregen).
    \[P_\lambda(k) = e^{-\lambda} \cdot \frac{\lambda^k}{k!}\]
  \item[Erwartungswert] \[E(X) = \lambda\]
  \item[Varianz] \[\text{var}(X) = \lambda\]
\end{description}

\subsection{Normalverteilung / Gaussverteilung (stetig)}
\begin{description}
  \item[Anwendung] Messfehler, Summe von vielen voneinander
  unabhängigenn Zufallsvariabeln
  \item[Dichtefunktion] \[\phi(x) = \frac{1}{\sigma \sqrt{2\pi}} \cdot
    e^{-\frac{1}{2}{\left(\frac{x-\mu}{\sigma}\right)}^2}\]
  \item[Verteilungsfunktion] \[E(X) = \mu\]
  \item[Varianz] \[\text{var}(X) = \sigma^2\]
\end{description}
