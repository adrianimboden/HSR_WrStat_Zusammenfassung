%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kombinatorik (Zählen)}
\subsection{Produktregel: Die Für–jedes–gibt–es–Regel (FJGE)}
Fur jede der $n_1$ Möglichkeiten gibt es eine von der
ersten Position unabhängige Anzahl $n_2$ Möglichkeiten für den Rest.
\[ = n_1 \cdot n_2 \text{ Möglichkeiten } \]

\subsection{Permutationen: Reihenfolge}
Auf wieviele Arten kann man $n$ Objekte anordnen? (Anzahl Anordnungen)
\[ = n! \text{ Arten} \]

\subsection{Kombinationen: Auswahl}
Auf wieviele Arten kann man $k$ Objekte aus $n$ auswählen?
\[ = C^n_k=\binom{n}{k} = \frac{n!}{k!(n-k)!} \text{ Arten} \]
Dieser "`Binomialkoeffizient"' lässt sich auf dem Taschenrechner
TI-36XII mit \texttt{n nCr k} (unter \texttt{PRB}), mit dem Voyage 200
mit \texttt{nCr(n,k)}, in Sage mit \texttt{binomial(n,k)} und in 
Octave/Matlab mit \texttt{nchoosek(n,k)} berechnen.

Auf wieviele Arten kann man $k$ Mal eine Auswahl aus $n$ Objekten
treffen?
\[ = n^k \text{ Arten} \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ereignisse und Wahrscheinlichkeit}
\subsection{Wahrscheinlichkeitsexperimente}
\begin{itemize}
  \item Alle möglichen Versuchsausgänge $\Omega$
  \item Elementarereignis $\omega \in \Omega$
  \item Ereignis $A \subset \Omega$
  \item Verschiedene Elementarereignisse $\omega \in A$
  \item Ereignis $A$ tritt ein $\Leftrightarrow \omega \in A$
  \item Wahrscheinlichkeit dass Ereignis $A$ eintritt ist $P(A)$
\end{itemize}

\subsection{Eigenschaften von Wahrscheinlichkeiten}
\begin{itemize}
  \item Laplace-Ereignisse: Alle Elementarereignisse haben die gleiche
    Wahrscheinlichkeit
  \item $P(A) = \frac{|A|}{|\Omega|}$: Wahrscheinlichkeit von
    Elementarereignis $A$
  \item $0 \le P(A) \le 1$: Wahrscheinlichkeit ist immer zwischen $0$
    und $1$
  \item $P(A) < P(B)$: Die Wahrscheinlichkeit für das Ereignis $A$ ist
    kleiner als für $B$
  \item $P(\Omega) = 1$: Das sichere Ereignis tritt immer ein
  \item $P(\emptyset) = 0$: Das unmöglich Ereignis tritt nie ein
\end{itemize}

\subsection{Rechnen mit Wahrscheinlichkeiten}
\begin{itemize}
  \item $P(A \cap B)$: Ereignis $A$ und Ereignis $B$ tritt ein
    \begin{itemize}
      \item Falls unabhängig: $= P(A) \cdot P(B)$
      \item Falls abhängig: Nicht alleine aus $P(A)$ und $P(B)$
        berechenbar!
    \end{itemize}
  \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$\footnote{Ein-
    Ausschaltformel}: Ereignis $A$ oder Ereignis $B$ tritt ein
    \begin{itemize}
      \item Falls sich $A$ und $B$ nicht überschneiden\footnote{
        Paarweise disjunkt: $A$ und $B$ treffen nicht gleichzeitig ein}:
        $= P(A) + P(B)$
      \item Drei Ereignisse: $P(A \cup B \cup C) \\ = P(A) + P(B) + P(C)
        - P(A \cap B) - P(A \cap C) - P(B \cap C) + P(A \cap B \cap C)$
    \end{itemize}
  \item $P(A \setminus B) = 1 - P(A)$: Ereignis $A$ tritt ein, aber ohne
    Ereignis $B$
  \item $P(\bar{A}) = P(\Omega \setminus A) = 1 - P(A)$: Ereignis $A$
    tritt nicht ein
  \begin{itemize}
    \item Bedingte Wahrscheinlichkeit: $P(\bar{A}|B) = 1 - P(A|B)$
  \end{itemize}
\end{itemize}

\subsection{Bedingte Wahrscheinlichkeit}
Wahrscheinlichkeit, dass $A$ eintritt, wenn wir schon wissen, dass $B$
eingetreten ist.\footnote{Somit gilt auch: $P(A) = P(A|\Omega)$}
\[ P(A|B) = \frac{P(A \cap B)}{P(B)} \]
Falls $A$ und $B$ unabhängig sind:
\[ P(A|B) = \frac{P(A) P(B)}{P(B)} = P(A) \]

\subsection{Satz der totalen Wahrscheinlichkeit}
Aus Einzelfällen kann man die Gesamtsituation zusammenstellen:
\[ P(A) = \sum_{i=1}^{n}P(A|B_i) \cdot P(B_i) \]

\subsection{Satz von Bayes}
Mit dem Satz von Bayes kann man die Schlussrichtung umkehren
\footnote{Weil $P(A|B) \cdot P(B) = P(A \cap B) = P(B|A) \cdot P(A)$}:
\[ P(A|B) = P(B|A) \cdot \frac{P(A)}{P(B)} \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Erwartungswert und Varianz}
\subsection{Erwartungswert}
\subsubsection{Begriffe}
\begin{itemize}
  \item Zufallsvariable $X$ ordnet Elementarereignissen $\omega$ Werte
    zu: $X: \Omega \rightarrow \mathbb{R}$
  \item Erwartungswert (Zufallsvariable im Mittel): \\
    $E(X) = \sum_{Werte} \text{Wert } \cdot
    \text{Wahrscheinlichkeit } = \sum_i w_i \cdot P(w_i)$
  \item Empirischer Erwartungswert = arithmetisches Mittel:
    $E(X) = \frac{1}{n} \sum_{i=1}^n x_i$
\end{itemize}
\subsubsection{Spezielle Zufallsvariable}
Charakteristische Funktion von $A$:
\[ \chi_A = \begin{cases}1 & w \in A \\ 0 & sonst \end{cases} \]
\[ E(X) = 0 \cdot P(\bar{A}) + 1 \cdot P(A) = P(A) \]

\subsubsection{Rechnen mit Erwartungswerten}
\begin{itemize}
  \item Multiplikation mit einem Faktor: $E(\lambda X) = \lambda E(X)$
  \item Addition zweier Zufallswerte: $E(X + Y) = E(X) + E(Y)$
  \item Produkt zweier Zufallswerte: $E(X \cdot Y) \ne E(X) \cdot E(Y)$
  \begin{itemize}
    \item Potenzieren (immer abhängig): $E(X^2) \ne E(X)^2$
    \item Wenn die zwei Werte sich nicht beeinflussen
      (unabhängig): $E(X \cdot Y) = E(X) \cdot E(Y)$
  \end{itemize}
\end{itemize}

\subsection{Varianz (Streubreite)}
\subsubsection{Begriffe}
\begin{itemize}
  \item Varianz (Mass für die mittlere Abweichung):
    $\text{var}(x) = E((X - E(X))^2) = E(X^2) - E(X)^2$
  \item Abstand ($\pm$) zum Erwartungswert: $\sqrt{\text{var}(x)}$
\end{itemize}
\subsubsection{Rechenregeln für Varianz}
\begin{itemize}
  \item Multiplikation mit einem Faktor:
    $\text{var}(\lambda X) = \lambda^2 \text{var}(X)$
  \item Addition (nur wenn $X$ und $Y$ unabhängig):
    $\text{var}(X+Y) = \text{var}(X) + \text{var}(Y)$
  \item Multiplikation: $\text{var}(X \cdot Y) = \text{var}(X)
    \text{var}(Y) + \text{var}(Y)E(X)^2 + \text{var}(X)E(Y)^2$
  \item Varianz einer Konstante: $\text{var}(c) = 0$
\end{itemize}

\subsection{Mittelwert}
\subsubsection{Begriffe}
\begin{itemize}
  \item Erwartungswert der Zufallsvariablen $X$: $\mu = E(X)$
  \item Varianz: Abweichung der Zufallsvariabeln von ihrem
    Erwartungswert: $X -\mu$
  \item Wie häufig überschreitet die Abweichung $\epsilon$?
  \item Wie wahrscheinlich ist es, dass die Abweichung gross ist?
    $P(|X - \mu| > \epsilon)$
  \item Faustregel: 10 mal mehr Genauigkeit = 100 mal mehr Arbeit.
\end{itemize}
\subsubsection{Ungleichung von Tschebyscheff}
Wahrscheinlichkeit, dass Zufallsvariable $X$ um mehr als $\epsilon$ vonm
Erwartungswert abweicht:
\[P(X|-\mu| > \epsilon) \le \frac{\text{var}(X)}{\epsilon^2} \]
\subsubsection{Wie gut ist der Mittelwert?}
\begin{itemize}
  \item Mittelwert: $M_n = \frac{X_1 + X_2 + \ldots + X_n}{n}$
  \item Erwartungswert: $E(X_i) = E\left( \frac{X_1 + X_2 + \ldots +
  X_n}{n}\right) =  \frac{E(X_1) + E(X_2) + \ldots + E(X_n)}{n} = \mu$
  \item Varianz: $\text{var}(X_i) = \sigma^2$
  \item Varianz: $\text{var}(M_n) = \frac{1}{n^2} \sum_{i=1}^n
  \text{var}(X_i) = \frac{\sigma^2}{n}$
\end{itemize}
\paragraph{Bernoullis Gesetz der grossen Zahlen}
Die Wahrscheinlichkeit, dass der Mittelwert von $n$ unabhängigen
Zufallsvariabeln mit Mittelwert $\mu$ und Varianz $\sigma^2$ mehr als
$\epsilon$ von $\mu$ abweicht, ist:
\[P(|X-\mu| > \epsilon) \le \frac{\text{var}(X)}{\epsilon^2} \]
Die Abweichung ist unwahrscheinlich, wenn:
\begin{itemize}
  \item Varianz ist klein (genaues Messgerät)
  \item $\epsilon$ ist gross (man ist toleranter)
  \item $n$ ist gross (viele Messungen)
\end{itemize}
